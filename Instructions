Student: Daniel Mateu
Course: Geotechnical Engineering — Dr. Song
Date: October 20, 2025

PROJECT OVERVIEW

This project will conduct a comparative analysis of multiple supervised machine learning algorithms to predict slope stability using a comprehensive dataset of slope case histories.1 The primary objective is to train and evaluate a suite of models—ranging from interpretable foundational models (Decision Tree, Support Vector Machine) to high-performance ensembles (Random Forest, XGBoost)—for a classification task (Stable/Unstable).1 The models will be trained and compared using a standard 80/20 data split. The robustness of the top-performing models will then be further evaluated by re-testing them on a more aggressive 70/30 split. The final analysis will identify the most influential geotechnical parameters and discuss the critical trade-offs between model accuracy, interpretability, and robustness.

KEY MILESTONES (WEEKLY)

Week 1 — Data Sourcing, Cleaning, & Exploratory Data Analysis (EDA)
Deliverables: Loaded & cleaned dataset, EDA plots (class balance, correlation heatmap).
Week 2 — Multi-Model Training & Robustness Testing
Deliverables: A comparative metrics table for all four models on an 80/20 split. A secondary table showing the performance of the champion model(s) on a 70/30 split to test for robustness.
Week 3 — In-Depth Interpretation & Analysis
Deliverables: Feature importance plots, partial dependence plots, and a confusion matrix for the "champion" model identified in Week 2.
Week 4 — Reporting and Presentation
Deliverables: Final PDF report, 8-slide deck, and a reproducible main.ipynb notebook with all supporting files.

TECHNICAL STACK

Environment/IDE: PyCharm
Python 3.x
Libraries: pandas, numpy, scikit-learn, matplotlib, seaborn, jupyter, openpyxl, xgboost

30-DAY (1 HOUR/DAY) ACTION PLAN


PHASE 1 — SETUP, DATA, & EDA (WEEK 1)

Day 1-2: Project & Environment Setup.
Action: Create PyCharm project, virtual environment, and requirements.txt file. Install all libraries. Create data folder and place your Dataset.xlsx file inside.
Day 3: Load & Clean Data.
Action: Create main.ipynb. Load your Dataset.xlsx file using pd.read_excel(). Clean the column names to be code-friendly (e.g., friction_angle_deg).
Day 4: Data Validation.
Action: Use df.info() and df.isnull().sum() to check for missing values and correct data types. Document findings in a Markdown cell.
Day 5: Engineer Target Variable.
Action: Create the is_stable binary column (1 for 'Stable', 0 for 'Failure') from the status column. Verify counts with df['is_stable'].value_counts().
Day 6: Plot Class Balance.
Action: Use seaborn.countplot() to visualize the distribution of Stable vs. Failure cases. Document the implications of any class imbalance.
Day 7: Plot Correlation Heatmap.
Action: Use df.corr() and seaborn.heatmap() to visualize relationships between features. Note the features most correlated with is_stable.

PHASE 2 — MULTI-MODEL TRAINING & ROBUSTNESS TESTING (WEEK 2)

Day 8: Data Preparation (80/20 Split).
Action: Define your feature matrix X and target vector y. Perform an 80/20 train-test split on the dataset. This will be your primary split for the main analysis.
Day 9: Feature Scaling.
Action: Use StandardScaler to create scaled versions of your 80/20 training and testing data (X_train_scaled, X_test_scaled). This is required for the SVM model.
Day 10: Model 1 & 2 (Decision Tree & SVM).
Action: Train and evaluate a DecisionTreeClassifier (on unscaled data) and a SVC (on scaled data). Record their performance metrics from the classification_report.
Day 11: Model 3 (Random Forest).
Action: Train and evaluate a RandomForestClassifier on the original (unscaled) training data. Record its performance.
Day 12: Model 4 (XGBoost).
Action: Train and evaluate an XGBClassifier on the training data. Record its performance.
Day 13: Consolidate 80/20 Results & Identify Champion(s).
Action: Create a Markdown table summarizing the key metrics (Precision, Recall, F1-Score) for all four models. Identify your "champion" model(s) based on the best performance.
Day 14: Robustness Check (70/30 Split).
Action: Create a new 70/30 split of the original data. Re-train only your champion model(s) on this new 70% training set and evaluate performance on the 30% test set. Create a second table comparing the champion model's performance on the 80/20 vs. 70/30 split.

PHASE 3 — IN-DEPTH INTERPRETATION & ANALYSIS (WEEK 3)

Day 15: Confusion Matrix.
Action: Plot the confusion matrix for your champion model (trained on the 80/20 split) using ConfusionMatrixDisplay. Analyze the types of errors, paying special attention to False Negatives.
Day 16: Feature Importance.
Action: Extract the .feature_importances_ attribute from your champion model (Random Forest or XGBoost from the 80/20 split).
Day 17: Plot Feature Importance.
Action: Create a horizontal bar chart to visualize the feature importances. This is a key result for your report.
Day 18: Partial Dependence Plots.
Action: Use PartialDependenceDisplay to plot the effect of the top 2 features on the model's prediction.
Day 19: Write "Insights" Summary.
Action: In a Markdown cell, write a summary connecting your feature importance findings to geotechnical principles (e.g., Mohr-Coulomb theory).
Day 20: Git Commit & Review.
Action: Commit your work with a message like "Completed multi-model analysis and robustness check." Review your notebook for clarity.
Day 21: Buffer Day.
Action: Use this day to catch up on any previous steps or to further refine your plots and analysis.

PHASE 4 — REPORTING & PRESENTATION (WEEK 4)

Day 22: Report - Methods Section.

Action: Write the Methods section, describing the dataset, preprocessing steps, all four models, and the 80/20 vs. 70/30 robustness check.
Day 23: Report - Results Section.
Action: Write the Results section. Include your main model comparison table, the robustness check table, and your key plots.
Day 24: Report - Discussion Section.
Action: Write the Discussion. Discuss the interpretability vs. accuracy trade-off (e.g., Decision Tree vs. XGBoost). Analyze your feature importance results. Crucially, discuss the model's robustness based on the split comparison.
Day 25: Complete Report.
Action: Write the Introduction and Conclusion. Proofread the entire document.
Day 26: Create Slide Deck.
Action: Create an 8-10 slide presentation. Include a dedicated slide for the "Robustness Check" to highlight your thorough analysis.
Day 27: Code Cleanup.
Action: Clean and comment your main.ipynb notebook. Ensure it runs from top to bottom without errors.
Day 28: Final Package Assembly.
Action: Polish your report and slides. Create the final .zip file for submission.
Day 29-30: Buffer Days.
Action: Use these days to catch up, practice your presentation, and do a final review of all deliverables.

